{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subjective-concern",
   "metadata": {},
   "source": [
    "# Implémentation d'un algorithme de Deep Q-Learning autour du problème de gestion de la circulation sur un carrefour automobile\n",
    "\n",
    "Prérequis : Connaissance en Q-Learning (sous-domaine du Reinforcement Learning/Apprentissage par renforcement), Deep Learning avec réseaux de neurones profonds, Package [Cityflow](https://github.com/cityflow-project/CityFlow) installé dans un environnement Anaconda dédié dans un os Linux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gross-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cityflow \n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-poker",
   "metadata": {},
   "source": [
    "### Initialisation de l'environnement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "periodic-breeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vintel38/MyCityFlow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adverse-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path=os.path.join(path,\"test1\",\"config.json\")\n",
    "# eng = cityflow.Engine(config_path, thread_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-worship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "light-powder",
   "metadata": {},
   "source": [
    "### Initialisation du buffer et de ses opérations possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liable-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque \n",
    "\n",
    "def sample_buffer(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, next_states, rewards = [np.array([experience[field_index] for experience in batch]) \n",
    "                                             for field_index in range(4)]\n",
    "    return states, actions, next_states, rewards\n",
    "\n",
    "def fill_buffer(experiences):\n",
    "    replay_buffer.append(experiences)\n",
    "    \n",
    "## random shuffling of numpy.random.shuffle of replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-mauritius",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "committed-control",
   "metadata": {},
   "source": [
    "### Initialisation de l'agent et de ses opérations possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescribed-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN(n_inputs, n_outputs):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(64, activation=\"relu\", input_shape=n_inputs),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(n_outputs)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def epsilon_policy(state, epsilon, n_outputs):\n",
    "    if epsilon > np.random.rand():\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "def play_step(eng, state, epsilon, n_outputs):\n",
    "    action = epsilon_policy(state, epsilon, n_outputs)\n",
    "    prereward=sum(np.array(list(eng.get_lane_waiting_vehicle_count().values())))\n",
    "    eng.next_step()\n",
    "    next_state=np.array(list(eng.get_lane_waiting_vehicle_count().values()))\n",
    "    reward=prereward-sum(next_state)\n",
    "    #print('len buffer before filling{}'.format(len(replay_buffer)))\n",
    "    fill_buffer((state, action, next_state, reward))\n",
    "    #print('len buffer after filling {}'.format(len(replay_buffer)))\n",
    "    return state, action, next_state, reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-leather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recorded-final",
   "metadata": {},
   "source": [
    "### Procédure d'entraînement \n",
    "\n",
    "Avant de commencer à entraîner le réseau de neurones à gérer la signalisation du carrefour, il est nécessaire de bien initialiser les parties prenantes de l'architecture et leurs interactions.\n",
    "\n",
    "L'état (state) est représenté par un vecteur colonne où chaque valeur correspond aux nombres de voitures à l'arrêt sur chaque ligne qui peut être représenté par la sortie de `eng.get_lane_waiting_vehicle_count()`. Ce vecteur a donc une taille de 1 x 12 dans notre cas d'intersection simplifiée. La récompense est définie comme le nombre de voiture mises en mouvement par intervalle de temps. Cette modélisation est simplifiée mais on peut vérifier si elle fonctionne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "likely-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "gamma=0.95 # discount factor\n",
    "optimizer=keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn=keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    states, actions, next_states, rewards = sample_buffer(batch_size)\n",
    "    \n",
    "    # calcul de la Qvalues target\n",
    "    Qval_futur = model.predict(next_states)\n",
    "    max_Qval_futur = np.max(Qval_futur, axis=1)\n",
    "    Qtarget = (rewards + gamma * max_Qval_futur)\n",
    "    \n",
    "    # calcul de la Qvalues training\n",
    "    mask= tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        Qval_actual = model(states)\n",
    "        Qtraining = tf.reduce_sum(Qval_actual*mask, axis=1, keepdims=True)\n",
    "        \n",
    "        # calcul de la loss\n",
    "        loss = tf.reduce_mean(loss_fn(Qtarget, Qtraining))\n",
    "        \n",
    "    # calcul des gradients et \n",
    "    # application de la Gradient Descent procedure sur les poids du réseau de neurones\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exceptional-aurora",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [ 0 24  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  0  0 24  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du buffer \n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "# Initialisation de l'environnement \n",
    "path = os.getcwd()\n",
    "config_path=os.path.join(path,\"test1\",\"config.json\")\n",
    "eng = cityflow.Engine(config_path, thread_num=1)\n",
    "\n",
    "# Initialisation du Neural Network\n",
    "n_inputs=24\n",
    "n_outputs=10 ## A REGLER\n",
    "# A faire : boucler l'action\n",
    "model = create_DNN([n_inputs],n_outputs)\n",
    "state = [0 for i in range(n_inputs)]\n",
    "\n",
    "eng.reset(seed=False)\n",
    "for episode in tqdm(range(100)):\n",
    "    epsilon = max(1-episode/100, 0.01)\n",
    "    state, action, next_state, reward = play_step(eng, state, epsilon, n_outputs)\n",
    "    state=next_state\n",
    "    \n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "print('state {}'.format(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "warming-bridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-registrar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
